# 질문
## 1) OpenAI/Gemini 사용 방식

### 권장 모델 조합(가성비/속도/정확 밸런스)

- **OpenAI (주력 응답 생성)**: `gpt-4.1-mini`
    - 고성능 대비 비용/지연 균형이 좋아 운영 기본 모델로 무난합니다.
- **OpenAI (고난도 에스컬레이션/정확도 필요 시)**: `gpt-4.1`
    - 복잡한 질의/근거 조합/긴 답변 품질이 필요할 때만 스위칭.
- **Gemini (초저지연/대량 트래픽용, 라우팅 또는 1차 응답)**: `gemini-2.5-flash-lite` (또는 preview 고정 버전)
    - fast + cost-efficiency + high throughput 포지션.
- **Gemini (복잡 추론/문서 대량 분석 필요 시)**: `gemini-2.5-pro`
    - 장문 컨텍스트/복잡 문제에 강한 thinking 모델.

> 운영 팁: 라우팅(경량 모델)  최종 생성(주력 모델)  필요 시 정제(상위 모델) 구조가 비용을 가장 잘 잡습니다.
> 

### SDK/REST 선호
**매우 중요 본 내용은 서비스의 흐름을 확인하고 서비스 흐름의 맥락과 맞지 않다면 무시하고, 서비스 흐름에 맞게 설계할 것**

- **권장**: 서버는 **SDK(공식 클라이언트)** 우선 + 장애 대비로 REST 백업
- **스트리밍**
    - **권장: ON**
    - UI 체감 속도(첫 토큰까지 시간)를 크게 개선
    - 서버는 **SSE/WebSocket**로 프론트에 그대로 전달

---

## 2) API 키/비밀값 보관 위치

### .env 변수명(권장 표준)

- `OPENAI_API_KEY=...`
- `GEMINI_API_KEY=...` (또는 Google AI Studio/VertexAI 방식에 맞게)

### 배포 환경 키 주입
**매우 중요 본 내용은 서비스의 흐름을 확인하고 서비스 흐름의 맥락과 맞지 않다면 무시하고, 서비스 흐름에 맞게 설계할 것**
- 로컬: `.env` (git ignore)
- 스테이징/프로덕션:
    - **K8s**: `Secret` + `envFrom`
    - **AWS**: Secrets Manager + task role 주입
    - **GCP**: Secret Manager + workload identity
- **절대 금지**
    - 프론트 번들에 키 포함
    - 로그에 Authorization 헤더/키 출력

---

## 3) KB RAG 정책 (임베딩/차원, 청킹, top-k, cutoff, 답변 포맷)

### 임베딩 모델 권장

- **OpenAI**: `text-embedding-3-small`(가성비) 또는 `text-embedding-3-large`(정확도)
    - 기본 차원: small=1536, large=3072
- **Google(Vertex/Gemini Embedding)**: `gemini-embedding-001`(최대 3072)
    - Gemini API 임베딩은 기본 3072, 필요 시 축소 권장(768/1536/3072)

> 운영 권장: 검색 품질이 목표면 임베딩은 large/3072 쪽이 유리하지만, 비용/저장소/속도 때문에 1536로 시작하는 팀이 많습니다.
> 

### Chunking 규칙(권장 기본값)

- **chunk_size**: 8001200 tokens
- **overlap**: 120200 tokens
- 문서 구조(제목/소제목/표/FAQ)는 **semantic splitter** 우선
- PDF/HTML은 문단 단위 + 헤더 유지가 검색 품질에 유리

### Retrieval 파라미터

- **top-k**: 610 (초기 8 추천)
- **rerank**: 가능하면 ON(특히 사내 KB 품질이 들쭉날쭉할 때)
- **score cutoff**
    - 벡터DB/스코어 정의마다 달라서 절대값 고정은 위험하지만,
    - 운영 정책은 **근거 부족 시 답변 금지/보류**로 두는 게 중요:
        - (예) top-k 중 유효 chunk가 2개 미만이면  추가 질문/에스컬레이션

### 답변 포맷(권장)

- 기본: **요약  근거(출처)  상세  다음 액션(필요 시)**
- 근거 기반 강제 규칙:
    - RAG 컨텍스트에 없는 내용은 **추측 금지**
    - 불확실하면 **질문 되묻기** 또는 근거 부족 고지

---

## 4) MCP 실행 규칙 (tool schema, adapter_key endpoint, 마스킹/레이트리밋)

### Tool schema 해석 방식(권장 룰)

- 모델이 반환한 tool call은 **엄격 검증**:
    1. JSON schema validation
    2. allowlist(tool_name, adapter_key)
    3. 인자 타입/길이/enum 검증
    4. PII/비밀값 필드 마스킹 규칙 적용
    5. 실행 전 권한/세션 상태 체크

### adapter_key별 endpoint 규격(운영 표준)

- `adapter_key`  `{base_url, auth_type, timeout_ms, retry, rate_limit}` 매핑 테이블로 관리
- endpoint 호출은 **서버에서만** 수행(프론트 직접 호출 금지)

### 마스킹/레이트리밋(권장)

- 마스킹:
    - access token, 주민번호/카드번호, 이메일/전화 일부 마스킹
    - 로그는 request/response를 요약 로그로 분리(원문 저장 금지 또는 암호화 저장)
- 레이트리밋:
    - 사용자 기준 + IP 기준 + tool 기준(외부 시스템 보호)
    - tool 에러 폭주 방지용 **circuit breaker** 권장

---

## 5) 세션 시작/종료 트리거 (/call/{token} 연동, 만족도 수집)

### /call/{token} 연동(권장 흐름)

- `/call/{token}` 진입 시:
    1. token 검증(만료/재사용/서명)
    2. 사용자/테넌트 식별
    3. 세션 생성(session_id) + 컨텍스트 로딩(최근 대화 요약/권한/프로필)
- 종료 트리거:
    - (A) 사용자가 종료/해결됨 클릭
    - (B) inactivity timeout(예: 1530분)
    - (C) 상담 전환(에스컬레이션) 시 자동 종료 + 티켓 생성

### 만족도 수집(키패드/웹)

- 웹: 👍/👎 + 이유 선택(정확도/속도/불친절/근거부족/기타)
- 키패드(ARS/콜센터):
    - 1~5 점수 + 0(상담원 연결)
- 저장 항목(최소):
    - session_id, 질문/답변 요약, 사용 모델, RAG 히트율(top-k 유효 개수), 만족도, 에러코드

---

## 6) mejai_Guideline 확인 루프(재질문 횟수/실패 시 에스컬레이션)

*(mejai_Guideline이 내부 규정/정책 문서라고 가정하고 검증 루프 운영 기준을 제안합니다.)*

### 재질문 횟수(권장)

- **최대 2회**
    - 1회: 핵심 정보(목표/범위/제약) 부족 시
    - 2회: 근거 부족/권한 부족/툴 실행 불가 시
- 2회 이후에도 불명확하면:
    - 제가 답을 확정할 수 없는 이유 + 필요한 정보 체크리스트를 제시하고 **에스컬레이션** 또는 답변 보류 처리

### 실패 시 에스컬레이션 기준(권장 트리거)

- **정책/법무/보안** 관련(고위험)
- RAG 근거가 임계치 미만(유효 chunk < 2 등)
- tool 호출 2회 실패(타임아웃/권한/외부 API 오류)
- 사용자 불만/반복 동일 질문(3회 이상)

---

### 운영 초기 기본값 한 줄 요약

- **OpenAI `gpt-4.1-mini` + (고난도만 `gpt-4.1`)**,
- **Gemini는 `gemini-2.5-flash-lite`로 라우팅/대량처리 + `gemini-2.5-pro`는 필요 시**,
- 임베딩은 **1536(가성비)** 또는 **3072(정확도)**로 시작.

#답변
확인했습니다. 제공한 기준을 우선 적용하되, 서비스 흐름과 충돌하는 부분은 mejai_Guideline과 현재 코드 구조에 맞춰 조정하겠습니다. 지금부터 모든 대화를 `reply/yyyy-mm-dd_hh-mm-ss.md` 형식으로 기록합니다. 이번 턴은 `reply/2026-01-28_16-26-52.md`에 저장했습니다.

반영 요약
- 모델 라우팅: 기본 `gpt-4.1-mini`, 고난도는 `gpt-4.1`, 저지연은 `gemini-2.5-flash-lite`, 복잡 분석은 `gemini-2.5-pro`
- 스트리밍: 서버에서 SSE/WebSocket으로 전달
- 키 관리: `OPENAI_API_KEY`, `GEMINI_API_KEY` 환경 변수 사용(프론트 노출 금지)
- RAG: 임베딩 1536 또는 3072, chunk/overlap/top-k/근거 부족 시 보류 규칙 적용
- MCP: schema 검증 → allowlist → 마스킹 → 정책 검사 → 호출 → 감사로그 기록
- mejai_Guideline 1~8 단계 기준으로 turns/event_logs/sessions에 실제 대화처럼 기록

진행을 위해 추가로 확인할 사항
1) OpenAI/Gemini API 호출 방식: SDK 사용 라이브러리(언어/버전), REST 백업 여부
2) 프론트 스트리밍 연결 방식: SSE 또는 WebSocket 중 무엇을 표준으로 할지
3) KB 임베딩 생성/업데이트 파이프라인: 어디서/언제 생성하며, 재인덱싱 트리거는 무엇인지
4) MCP adapter_key별 실제 외부 API 규격(기본 URL, auth_type, 요청/응답 스키마)
5) mejai_Guideline의 대화 기록 포맷 상세(예: 어떤 텍스트를 turns의 어떤 필드에 저장할지)

이 정보 주시면 LLM/MCP/KB 통합을 실제 구현으로 진행하겠습니다.
