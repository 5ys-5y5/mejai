# 질문
- LLM SDK 설치 승인: openai, @google/generative-ai (네트워크 접근 필요): 설치를 승인합니다.
- 주문 조치 범위 확정: 현재 MCP로 실행 가능한 범위로만 한정합니다.
- KB 임베딩 파이프라인 필요 여부 확정: 질문 자체를 이해하지 못했습니다. 자세히 설명해주세요

#답변
승인된 SDK 설치를 완료했습니다. `openai`, `@google/generative-ai`가 추가되었습니다.

KB 임베딩 파이프라인 설명
- KB 문서마다 “벡터(embedding)”를 만들어 `knowledge_base.embedding` 컬럼에 저장하는 흐름을 의미합니다.
- 이 벡터를 기반으로 유사도 검색(RAG)을 수행해 관련 문서를 찾아 LLM에 근거로 제공합니다.
- 임베딩이 없으면:
  - RAG 검색이 불가능하거나,
  - 키워드 검색만 되며 정확도가 크게 떨어집니다.

임베딩 파이프라인 구성 옵션
1) **작성/수정 시 즉시 생성(동기식)**  
   - 문서 저장 요청 시 embedding 생성 후 저장  
   - 장점: 항상 최신, 단점: 저장 지연
2) **작성/수정 시 큐/배치(비동기식)**  
   - 저장은 즉시, 임베딩은 백그라운드 처리  
   - 장점: 빠름, 단점: 잠시 RAG 공백
3) **온디맨드 생성**  
   - 질문 들어올 때 해당 KB에 임베딩이 없으면 생성  
   - 장점: 초기 작업 적음, 단점: 첫 응답 지연

결정이 필요합니다
- 현재 `knowledge_base.embedding`에 이미 값이 들어가 있나요?
- 없다면 위 1/2/3 중 어떤 방식을 선택할까요?
